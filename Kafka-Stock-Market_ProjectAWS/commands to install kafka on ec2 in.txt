commands to install kafka on ec2 instance:

[ec2-user@ip-172-31-45-24 ~]$ wget https://downloads.apache.org/kafka/3.3.2/kafka_2.12-3.3.2.tgz

tar -xvf kafka_2.12-3.3.2.tgz

ec2 update into amazon linux 2023

java installation
instead of yum now it is dnf

[ec2-user@ip-172-31-45-24 ~]$ sudo dnf install java-1.8.0-amazon-corretto

[ec2-user@ip-172-31-45-24 kafka_2.12-3.3.2]$ bin/zookeeper-server-start.sh config/zookeeper.properties

*** now when i want to start kafka server , then i need to again to connect to my instance with a new terminal
OpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000c0000000, 1073741824, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (mmap) failed to map 1073741824 bytes for committing reserved memory.
# An error report file with more information is saved as:
# /home/ec2-user/kafka_2.12-3.3.2/hs_err_pid25826.log

getting this error, means we need to increase the memory for our . because we are running our kafka whole server in a single ec2 machine so we need to assign more memory to it...

*** export KAFKA_HEAP_OPTS="-Xmx256M -Xms128M"

[ec2-user@ip-172-31-33-245 kafka_2.12-3.3.2]$ bin/kafka-server-start.sh config/server.properties

** after starting kafka server , it starts on private dns, we can't access it to send data , unless we are working on the same network, send data by using other network we need to change private dns into public...**

changing the server properties---
[ec2-user@ip-172-31-33-245 kafka_2.12-3.3.2]$ sudo nano config/server.properties

** change in advertised listener to public dns on aws
to save press cltr+x ->y->enter we can save this file

**
creating a topic
[ec2-user@ip-172-31-47-210 bin]$ ./kafka-topics.sh --create --topic awskafka --bootstrap-server 13.233.132.142:9092 --replication-factor 1 --partitions 1

[ec2-user@ip-172-31-47-210 bin]$ ./kafka-topics.sh --bootstrap-server 13.233.132.142:9092 --list
aws_kafka


** after doing this part, and creating your own producer and consumer in python, 
our first part is done, dataset real time, python-kafka producer - ec2 kafka machine - python-kafka-consumer 

** now, we need to dumb this amazon s3, create a bucket and then, we need to install s3fs and configure the aws in local machine.. by IAM 

we created a bucket and dump the data into it via python, 

and then we use crawler to easily crawl the data, so we can directly query by athena, 

so we create crawler, by user, and crawler role and inside of it database, , crawler role,-giving the permission by iam to the glue, means it can handle the read write permission of my amazon s3 bucket service, 

crawler and glue catalog it's done, then athena also, save query results of athena by creating and s3 bucket and give the path, but make sure on the same region


**** database drop in athena, if you use a name with hyphen , use backticks 
to drop schema first and then drop database:

DROP
 SCHEMA IF 
EXISTS
 `stock
-
market
-
kafka` CASCADE;

DROP SCHEMA 
`stock-market-kafka`
 CASCADE;  to drop the database, if it is showing still 
